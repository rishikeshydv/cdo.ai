model_name: meta-llama/Meta-Llama-3.1-70B-Instruct
train_jsonl: data/sft_train.jsonl
val_jsonl: data/sft_val.jsonl
out_dir: artifacts/checkpoints/sft_phase1

# Set this from token_stats.py (p95)
max_seq_len: 8192

lr: 5.0e-6
epochs: 1.0
seed: 1337

per_device_train_batch_size: 1
gradient_accumulation_steps: 16

warmup_ratio: 0.05
logging_steps: 10
eval_steps: 100
save_steps: 200
